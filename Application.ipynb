{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dstk.utils.data_cleaning import clean_columns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_columns = 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FILES:\n",
    "\n",
    "    - application_test.csv.zip\n",
    "    - application_train.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_des = pd.read_csv('HomeCredit_columns_description.csv', encoding='latin-1')\n",
    "col_des.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_des.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_des.Table.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    " \n",
    " \n",
    "   - Examine Distribution of Target (check for imbalanced classes)\n",
    "   - Examine Column Types\n",
    "   - Remove/Impute Anomalies\n",
    "     - You can also create a categorical value (0,1) for whether or not the data was anomalous.\n",
    "   - Fill Missing Variables\n",
    "   - Label Encode binary features and OHE multiple categorical ftrs\n",
    "     - _Make sure to drop categories that are not in the test set!_\n",
    "     - check out df.align?\n",
    "   - Look for correlations\n",
    "   - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Main File\n",
    "\n",
    "Contains main id. 1 Row is 1 loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train = pd.read_csv('application_train.csv.zip', dtype= {'SK_ID_CURR':str})\n",
    "app_test = pd.read_csv('application_test.csv.zip', dtype= {'SK_ID_CURR':str})\n",
    "app_train.shape, app_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(\n",
    "    zip(col_des[col_des.Table=='application_{train|test}.csv'].Row.tolist(),col_des[col_des.Table=='application_{train|test}.csv'].Description.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Well fuck\n",
    "app_train.TARGET.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train.groupby('CODE_GENDER').TARGET.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(app_train.loc[app_train.TARGET==0, 'DAYS_BIRTH']/-365, label = 'Repayed')\n",
    "sns.kdeplot(app_train.loc[app_train.TARGET==1, 'DAYS_BIRTH']/-365, label = 'Defaulted')\n",
    "plt.xlabel('Age Yrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean columns\n",
    "app_train.columns = clean_columns(app_train)\n",
    "app_test.columns = clean_columns(app_test)\n",
    "app_train.shape, app_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_descrip(table, col):\n",
    "    \"Looks up column description for given table\"\n",
    "    print(table)\n",
    "    print(col)\n",
    "    return col_des.loc[(col_des.Table==table) & (col_des.Row==col.upper()), \n",
    "            'Description'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pk = ['sk_id_curr']\n",
    "tgt = ['target']\n",
    "obj_cols = app_train.dtypes[app_train.dtypes=='object'].index.drop(pk).tolist()\n",
    "int_cols = app_train.dtypes[app_train.dtypes=='int64'].index.tolist()\n",
    "float_cols = app_train.dtypes[app_train.dtypes=='float64'].index.tolist()\n",
    "indicator_cols = []\n",
    "\n",
    "\n",
    "numeric_cols = int_cols + float_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some columns are already encoded\n",
    "indicator_cols.extend(app_train[numeric_cols].nunique()[app_train[numeric_cols].nunique() == 2].index.tolist())\n",
    "int_cols = [i for i in int_cols if i not in indicator_cols]\n",
    "float_cols = [i for i in float_cols if i not in indicator_cols]\n",
    "\n",
    "\n",
    "numeric_cols = int_cols + float_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target column separate\n",
    "for c in tgt:\n",
    "    indicator_cols.remove(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not sure what to do about these :-/\n",
    "# NOTE nunique() does NOT count nans!\n",
    "app_train[numeric_cols].nunique()[app_train[numeric_cols].nunique() < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_descrip('application_{train|test}.csv', 'region_rating_client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_descrip('application_{train|test}.csv', 'amt_req_credit_bureau_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_descrip('application_{train|test}.csv', 'amt_req_credit_bureau_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train['amt_req_credit_bureau_day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pk) + len(tgt) + len(obj_cols) + len(indicator_cols) + len(int_cols) + len(float_cols), app_train.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order the columns\n",
    "app_train = app_train[pk + tgt + sorted(obj_cols) + sorted(indicator_cols) + sorted(int_cols) + sorted(float_cols)]\n",
    "app_test = app_test[pk + sorted(obj_cols) + sorted(indicator_cols) + sorted(int_cols) + sorted(float_cols)]\n",
    "app_train.shape, app_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all columns that have ANY negative numbers\n",
    "neg_cols = app_train[numeric_cols].loc[:,(app_train[numeric_cols] < 0).any(0)].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make positive for interpretation sake\n",
    "app_train[neg_cols] = app_train[neg_cols] * -1\n",
    "app_test[neg_cols] = app_test[neg_cols] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Days employed has some negative some positive values\n",
    "app_train[numeric_cols].loc[:,(app_train[numeric_cols] < 0).any(0)].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiply just neg values by -1\n",
    "app_train.loc[app_train['days_employed']<0, 'days_employed'] = app_train.loc[app_train['days_employed']<0, 'days_employed'] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anom_eval(df, col, filename):\n",
    "    print(col_descrip(filename, col))\n",
    "    print()\n",
    "\n",
    "    max_diff_idx = df[col].sort_values().diff().nlargest(3).index\n",
    "    max_diffs = df.loc[max_diff_idx, col]\n",
    "    nlargest = df[col].nlargest()\n",
    "    nsmallest = df[col].nsmallest()\n",
    "\n",
    "    print('Max Diffs')\n",
    "    print(max_diffs)\n",
    "    print()\n",
    "    print(\"Largest Vals\")\n",
    "    print(nlargest)\n",
    "    print()\n",
    "    print(\"Smallest Vals\")\n",
    "    print(nsmallest)\n",
    "    df[col].hist()\n",
    "    df[col].value_counts().sort_index()\n",
    "    \n",
    "    return max_diffs, nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# col = 'amt_income_total'\n",
    "# app_train[col].hist()\n",
    "\n",
    "# sns.boxplot(app_train[col], whis=10)\n",
    "\n",
    "# p25 = np.percentile(app_train[col], 25)\n",
    "# p75 = np.percentile(app_train[col], 75)\n",
    "# iqr = p75-p25\n",
    "\n",
    "# max_val = p75 + 10*iqr\n",
    "# min_val = p25 - 10*iqr\n",
    "\n",
    "# app_train.loc[(app_train[col]>max_val) | (app_train[col]<min_val), col].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify absurd outliers by looking at large jumps in data. \n",
    "\n",
    "These are for distributions where there are a subset of values WAY outside the normal range such as days_employed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gaps = app_train[numeric_cols].apply(lambda ser: ser.sort_values().diff().max()/ser.std())\n",
    "gaps[gaps>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From manual exploration, these are the columns with ridiculous outliers\n",
    "cols = ['cnt_children',\n",
    "        'days_employed',\n",
    "        'amt_income_total',\n",
    "        'cnt_fam_members',\n",
    "        'obs_30_cnt_social_circle',\n",
    "        'def_30_cnt_social_circle',\n",
    "        'obs_60_cnt_social_circle',\n",
    "        'def_60_cnt_social_circle',\n",
    "        'amt_req_credit_bureau_qrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anom_eval(app_train, 'cnt_children', 'application_{train|test}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    print(col)\n",
    "    max_diff_idx = app_train[col].sort_values().diff().nlargest(1).index\n",
    "    max_diff = app_train.loc[max_diff_idx, col].squeeze()\n",
    "    denom = int(np.log10(max_diff))\n",
    "    \n",
    "    if denom < 2:\n",
    "        # if we are only in the 10s palce leave it\n",
    "        cutoff = max_diff\n",
    "    else:\n",
    "        # if we are greater than the 10s place round down\n",
    "        cutoff = int(max_diff/(10**denom))*(10**denom)\n",
    "    \n",
    "    anoms_train = app_train.loc[app_train[col]>=cutoff, col]\n",
    "    anoms_test = app_test.loc[app_test[col]>=cutoff,col]\n",
    "    # fill with vals from train set\n",
    "    rest = app_train.loc[~app_train.index.isin(anoms_train.index), col]\n",
    "    \n",
    "    # replace outliers with median from non-outlying training data\n",
    "    app_train.loc[anoms_train.index, col] = rest.median()\n",
    "    app_test.loc[anoms_test.index, col] = rest.median()\n",
    "    \n",
    "    # create identifier for anomalies\n",
    "    app_train[col+'_anom'] = 0\n",
    "    app_test[col+'_anom'] = 0\n",
    "    app_train.loc[anoms_train.index, col+'_anom'] = 1\n",
    "    app_test.loc[anoms_test.index, col+'_anom'] = 1\n",
    "    \n",
    "    # add to indicator columns\n",
    "    indicator_cols.append(col+'_anom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pct_null(df):\n",
    "    null_counts = df.isnull().sum()[df.isnull().sum()>0].sort_values(ascending=False).to_frame()\n",
    "    if null_counts.empty:\n",
    "        return None\n",
    "    null_counts['pct_null'] = null_counts/df.shape[0]\n",
    "    null_counts.columns = ['n_null', 'pct_null']\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# good that no null primary keys\n",
    "app_train[pk].isnull().sum(), app_test[pk].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns where less than 20% is null, i'm filling with the most-frequent value\n",
    "\n",
    "For columns where most is null i'm filling with \"missing\" and this will get encoded as it's own category.\n",
    "\n",
    "__NOTE__ A _better_ strategy would be to compute how much the target distributions differ for rows where the column is null and if it is distinctly different then label as \"missing\" so that it is predictive. Otherwise fill with most-frequent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_OBJECT COLS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "obj_nulls = pct_null(app_train[obj_cols])\n",
    "fill_most_frequent = obj_nulls[obj_nulls.pct_null < 0.2].index.tolist()\n",
    "fill_missing = obj_nulls.index.drop(fill_most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modes = app_train[fill_most_frequent].mode().T.to_dict()[0]\n",
    "app_train.fillna(modes, inplace=True)\n",
    "app_train.fillna(dict.fromkeys(fill_missing,'MISSING'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "obj_nulls = pct_null(app_test[obj_cols])\n",
    "fill_most_frequent = obj_nulls[obj_nulls.pct_null < 0.2].index.tolist()\n",
    "fill_missing = obj_nulls.index.drop(fill_most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note i'm filling with most-frequents from train data even for test missings\n",
    "modes = app_train[fill_most_frequent].mode().T.to_dict()[0]\n",
    "app_test.fillna(modes, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_test.fillna(dict.fromkeys(fill_missing,'MISSING'), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NUMERIC COLS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for numeric cols, just fill w median\n",
    "medians = app_train[numeric_cols].median().squeeze()\n",
    "app_train.fillna(medians, inplace=True)\n",
    "app_test.fillna(medians, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Variables To Numeric\n",
    "\n",
    "Label Encode binary categoricals and OHE other categoricals with multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first make sure no columns have only 1 value\n",
    "app_train.columns[app_train.nunique() < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_cols = app_train[obj_cols].nunique()[app_train[obj_cols].nunique() ==2].index.tolist()\n",
    "binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lcoder = LabelEncoder()\n",
    "for col in binary_cols:\n",
    "    print(col)\n",
    "    # Check to make sure it's binary accross both test and train\n",
    "    if not set(app_train[col].unique()) == set(app_test[col].unique()):\n",
    "        print(f'{col} is not Binary!')\n",
    "        print('Values in train:', set(app_train[col].unique()))\n",
    "        print('Values in test:', set(app_train[col].unique()))\n",
    "        continue\n",
    "    app_train[col] = lcoder.fit_transform(app_train[col])\n",
    "    app_test[col] = lcoder.transform(app_test[col])\n",
    "    # don't forget to add it to indicator cols\n",
    "    indicator_cols.append(col)\n",
    "    obj_cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encode\n",
    "non_binary_cats = [col for col in obj_cols if col not in binary_cols]\n",
    "non_binary_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in non_binary_cats:\n",
    "    print(col)\n",
    "    ohe_train = pd.get_dummies(app_train[col]) # drop_first?\n",
    "    ohe_train.columns = col + '_' + ohe_train.columns\n",
    "    \n",
    "    ohe_test = pd.get_dummies(app_test[col]) # they might not align if I drop_first...\n",
    "    ohe_test.columns = col + '_' + ohe_test.columns\n",
    "    ohe_test = ohe_test.align(ohe_train, 'left', 1)[0].fillna(0)\n",
    "    \n",
    "    # now drop first\n",
    "    ohe_train.drop(ohe_train.columns[0], axis=1, inplace=True)\n",
    "    ohe_test.drop(ohe_train.columns[0], axis=1, inplace=True) # should be the same first column, but just incase\n",
    "    \n",
    "    app_train = pd.concat([app_train, ohe_train], axis=1)\n",
    "    app_test = pd.concat([app_test, ohe_test], axis=1)\n",
    "    print(app_train.shape, app_test.shape)\n",
    "    del app_train[col]\n",
    "    del app_test[col]\n",
    "    obj_cols.remove(col)\n",
    "    indicator_cols.extend(ohe_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train.to_csv('clean_data/app_train.csv',index=False)\n",
    "app_test.to_csv('clean_data/app_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
